{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training/Batch transform/processing job/endpoint.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNsKMtYyPibsZ+qUYhvQ2Dx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joykareko/AWS-Udacity/blob/main/Training_Batch_transform_processing_job_endpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Job"
      ],
      "metadata": {
        "id": "WbgYSLXWM8Vy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-NGtGfugQ9i"
      },
      "outputs": [],
      "source": [
        "  #Training Job\n",
        "  import sagemaker\n",
        "from sagemaker import get_execution_role\n",
        "from sagemaker import image_uris\n",
        "from sagemaker.predictor import csv_serializer\n",
        "\n",
        "session = sagemaker.Session()\n",
        "\n",
        "role = get_execution_role()\n",
        "\n",
        "# If you're following along, you'll need to upload these datasets to your own bucket in S3. \n",
        "\n",
        "test_location = 's3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/test.csv'\n",
        "val_location = 's3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/validation.csv'\n",
        "train_location = 's3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/train.csv'\n",
        "\n",
        "# We use this prefix to help us determine where the output will go. \n",
        "\n",
        "prefix = 's3://sagemaker-us-west-2-565094796913/'\n",
        "\n",
        "# We need to get the location of the container. \n",
        "\n",
        "container = image_uris.retrieve('xgboost', session.boto_region_name, version='latest')\n",
        "\n",
        "# Now that we know which container to use, we can construct the estimator object.\n",
        "xgb = sagemaker.estimator.Estimator(container, # The image name of the training container\n",
        "                                    role,      # The IAM role to use (our current role in this case)\n",
        "                                    instance_count=1, # The number of instances to use for training\n",
        "                                    instance_type='ml.m4.xlarge', # The type of instance to use for training\n",
        "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
        "                                                                        # Where to save the output (the model artifacts)\n",
        "                                    sagemaker_session=session) # The current SageMaker session\n",
        "             \n",
        "# These hyperparameters are beyond the scope of this course, but you can research the algoirthm here: \n",
        "# https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html    \n",
        "    \n",
        "xgb.set_hyperparameters(max_depth=5,\n",
        "                        eta=0.2,\n",
        "                        gamma=4,\n",
        "                        min_child_weight=6,\n",
        "                        subsample=0.8,\n",
        "                        objective='reg:linear',\n",
        "                        early_stopping_rounds=10,\n",
        "                        num_round=200)\n",
        "                        \n",
        "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')\n",
        "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data=val_location, content_type='csv')\n",
        "\n",
        "# The fit method launches the training job. \n",
        "\n",
        "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Endpoint"
      ],
      "metadata": {
        "id": "m20jszhjNKIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#endpoint\n",
        "from sagemaker import get_execution_role\n",
        "from sagemaker.model import Model\n",
        "from sagemaker import image_uris\n",
        "\n",
        "role = get_execution_role()\n",
        "\n",
        "# You'll need to confirm that this region is located in the same place as the S3 uri of your training job.\n",
        "# (Check the upper right-hand side of the console.)\n",
        "\n",
        "image_uri = image_uris.retrieve(framework='xgboost',region='us-west-2', version='latest')\n",
        "\n",
        "# You'll need to replace this model data with the output S3 uri of your training job. \n",
        "\n",
        "model_data = \"s3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/output/xgboost-2021-08-31-23-02-30-970/output/model.tar.gz\"\n",
        "\n",
        "model = Model(image_uri=image_uri, model_data=model_data, role=role)\n",
        "\n",
        "predictor = model.deploy(initial_instance_count=1, instance_type=\"ml.m5.large\")\n"
      ],
      "metadata": {
        "id": "OMlK3DYaNE2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Transform Job"
      ],
      "metadata": {
        "id": "-5-_I6WQNY2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker import get_execution_role\n",
        "from sagemaker.model import Model\n",
        "from sagemaker import image_uris\n",
        "\n",
        "role = get_execution_role()\n",
        "\n",
        "# You'll need to confirm that this region is located in the same place as the S3 uri of your training job.\n",
        "# (Check the upper right-hand side of the console.)\n",
        "\n",
        "image_uri = image_uris.retrieve(framework='xgboost',region='us-west-2', version='latest')\n",
        "\n",
        "# You'll need to replace this with the output uri of a training job. \n",
        "\n",
        "model_data = \"s3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/output/xgboost-2021-08-31-23-02-30-970/output/model.tar.gz\"\n",
        "\n",
        "# You'll need to replace this with the desired output of your batch transform job. \n",
        "\n",
        "batch_transform_output_path = \"s3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/test_batch_output-2\"\n",
        "\n",
        "model = Model(image_uri=image_uri, model_data=model_data, role=role)\n",
        "\n",
        "transformer = model.transformer(\n",
        "    instance_count=1,\n",
        "    instance_type='ml.m4.xlarge',\n",
        "    output_path=batch_transform_output_path\n",
        ")\n",
        "\n",
        "# You'll need to replace the output data with your S3 uri of your dataset in S3. \n",
        "\n",
        "transformer.transform(\n",
        "    data=\"s3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/test.csv\",\n",
        "    data_type='S3Prefix',\n",
        "    content_type='text/csv',\n",
        "    split_type='Line'\n",
        ")\n"
      ],
      "metadata": {
        "id": "7eTIz3UENUcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing Job"
      ],
      "metadata": {
        "id": "JsB4w1dcNlpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Processing job\n",
        "%%writefile xgboost_process_script.py\n",
        "\n",
        "# Execute this cell first to write this script to your local directory. \n",
        "\n",
        "import pandas\n",
        "\n",
        "# This method filters out the column at index 1, which is the crime data. \n",
        "\n",
        "def filter_crime_data(input_data_path):\n",
        "    with open(input_data_path, 'r') as f:\n",
        "        df = pandas.read_csv(f)\n",
        "    df.drop(df.columns[[1]], axis=1)\n",
        "    return df\n",
        "\n",
        "# The main method takes in data at '/opt/ml/processing/input/data/train.csv' \n",
        "# and outputs it as a csv to '/opt/ml/processing/output/data_processed'\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    filtered_data = filter_crime_data('/opt/ml/processing/input/data/train.csv')\n",
        "    filtered_data.to_csv('/opt/ml/processing/output/data_processed')\n",
        "\n"
      ],
      "metadata": {
        "id": "ikkYI-AZNdgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "\n",
        "from sagemaker import get_execution_role\n",
        "from sagemaker.sklearn.processing import SKLearnProcessor\n",
        "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
        "\n",
        "role = get_execution_role()\n",
        "\n",
        "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
        "                                     role=role,\n",
        "                                     instance_type='ml.m5.large',\n",
        "                                     instance_count=1)\n",
        "\n",
        "\n",
        "# You will need to replace the 'source' code with the location of the dataset you want to process. \n",
        "\n",
        "sklearn_processor.run(code='xgboost_process_script.py',\n",
        "                        inputs=[ProcessingInput(\n",
        "                        source='s3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/train.csv',\n",
        "                        destination='/opt/ml/processing/input/data/')],\n",
        "                      outputs=[ProcessingOutput(source='/opt/ml/processing/output')]\n",
        "                     )\n"
      ],
      "metadata": {
        "id": "ivCa-ePLNzQ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
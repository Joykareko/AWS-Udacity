{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training/Batch transform/processing job/endpoint.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNx2nI7DfO3WjAoENYpPiai",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joykareko/AWS-Udacity/blob/main/Training_Batch_transform_processing_job_endpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Job"
      ],
      "metadata": {
        "id": "WbgYSLXWM8Vy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-NGtGfugQ9i"
      },
      "outputs": [],
      "source": [
        "  #Training Job\n",
        "  import sagemaker\n",
        "from sagemaker import get_execution_role\n",
        "from sagemaker import image_uris\n",
        "from sagemaker.predictor import csv_serializer\n",
        "\n",
        "session = sagemaker.Session()\n",
        "\n",
        "role = get_execution_role()\n",
        "\n",
        "# If you're following along, you'll need to upload these datasets to your own bucket in S3. \n",
        "\n",
        "test_location = 's3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/test.csv'\n",
        "val_location = 's3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/validation.csv'\n",
        "train_location = 's3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/train.csv'\n",
        "\n",
        "# We use this prefix to help us determine where the output will go. \n",
        "\n",
        "prefix = 's3://sagemaker-us-west-2-565094796913/'\n",
        "\n",
        "# We need to get the location of the container. \n",
        "\n",
        "container = image_uris.retrieve('xgboost', session.boto_region_name, version='latest')\n",
        "\n",
        "# Now that we know which container to use, we can construct the estimator object.\n",
        "xgb = sagemaker.estimator.Estimator(container, # The image name of the training container\n",
        "                                    role,      # The IAM role to use (our current role in this case)\n",
        "                                    instance_count=1, # The number of instances to use for training\n",
        "                                    instance_type='ml.m4.xlarge', # The type of instance to use for training\n",
        "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
        "                                                                        # Where to save the output (the model artifacts)\n",
        "                                    sagemaker_session=session) # The current SageMaker session\n",
        "             \n",
        "# These hyperparameters are beyond the scope of this course, but you can research the algoirthm here: \n",
        "# https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html    \n",
        "    \n",
        "xgb.set_hyperparameters(max_depth=5,\n",
        "                        eta=0.2,\n",
        "                        gamma=4,\n",
        "                        min_child_weight=6,\n",
        "                        subsample=0.8,\n",
        "                        objective='reg:linear',\n",
        "                        early_stopping_rounds=10,\n",
        "                        num_round=200)\n",
        "                        \n",
        "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')\n",
        "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data=val_location, content_type='csv')\n",
        "\n",
        "# The fit method launches the training job. \n",
        "\n",
        "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Endpoint"
      ],
      "metadata": {
        "id": "m20jszhjNKIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#endpoint\n",
        "from sagemaker import get_execution_role\n",
        "from sagemaker.model import Model\n",
        "from sagemaker import image_uris\n",
        "\n",
        "role = get_execution_role()\n",
        "\n",
        "# You'll need to confirm that this region is located in the same place as the S3 uri of your training job.\n",
        "# (Check the upper right-hand side of the console.)\n",
        "\n",
        "image_uri = image_uris.retrieve(framework='xgboost',region='us-west-2', version='latest')\n",
        "\n",
        "# You'll need to replace this model data with the output S3 uri of your training job. \n",
        "\n",
        "model_data = \"s3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/output/xgboost-2021-08-31-23-02-30-970/output/model.tar.gz\"\n",
        "\n",
        "model = Model(image_uri=image_uri, model_data=model_data, role=role)\n",
        "\n",
        "predictor = model.deploy(initial_instance_count=1, instance_type=\"ml.m5.large\")\n"
      ],
      "metadata": {
        "id": "OMlK3DYaNE2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Transform Job"
      ],
      "metadata": {
        "id": "-5-_I6WQNY2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker import get_execution_role\n",
        "from sagemaker.model import Model\n",
        "from sagemaker import image_uris\n",
        "\n",
        "role = get_execution_role()\n",
        "\n",
        "# You'll need to confirm that this region is located in the same place as the S3 uri of your training job.\n",
        "# (Check the upper right-hand side of the console.)\n",
        "\n",
        "image_uri = image_uris.retrieve(framework='xgboost',region='us-west-2', version='latest')\n",
        "\n",
        "# You'll need to replace this with the output uri of a training job. \n",
        "\n",
        "model_data = \"s3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/output/xgboost-2021-08-31-23-02-30-970/output/model.tar.gz\"\n",
        "\n",
        "# You'll need to replace this with the desired output of your batch transform job. \n",
        "\n",
        "batch_transform_output_path = \"s3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/test_batch_output-2\"\n",
        "\n",
        "model = Model(image_uri=image_uri, model_data=model_data, role=role)\n",
        "\n",
        "transformer = model.transformer(\n",
        "    instance_count=1,\n",
        "    instance_type='ml.m4.xlarge',\n",
        "    output_path=batch_transform_output_path\n",
        ")\n",
        "\n",
        "# You'll need to replace the output data with your S3 uri of your dataset in S3. \n",
        "\n",
        "transformer.transform(\n",
        "    data=\"s3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/test.csv\",\n",
        "    data_type='S3Prefix',\n",
        "    content_type='text/csv',\n",
        "    split_type='Line'\n",
        ")\n"
      ],
      "metadata": {
        "id": "7eTIz3UENUcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing Job"
      ],
      "metadata": {
        "id": "JsB4w1dcNlpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Processing job\n",
        "%%writefile xgboost_process_script.py\n",
        "\n",
        "# Execute this cell first to write this script to your local directory. \n",
        "\n",
        "import pandas\n",
        "\n",
        "# This method filters out the column at index 1, which is the crime data. \n",
        "\n",
        "def filter_crime_data(input_data_path):\n",
        "    with open(input_data_path, 'r') as f:\n",
        "        df = pandas.read_csv(f)\n",
        "    df.drop(df.columns[[1]], axis=1)\n",
        "    return df\n",
        "\n",
        "# The main method takes in data at '/opt/ml/processing/input/data/train.csv' \n",
        "# and outputs it as a csv to '/opt/ml/processing/output/data_processed'\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    filtered_data = filter_crime_data('/opt/ml/processing/input/data/train.csv')\n",
        "    filtered_data.to_csv('/opt/ml/processing/output/data_processed')\n",
        "\n"
      ],
      "metadata": {
        "id": "ikkYI-AZNdgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "\n",
        "from sagemaker import get_execution_role\n",
        "from sagemaker.sklearn.processing import SKLearnProcessor\n",
        "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
        "\n",
        "role = get_execution_role()\n",
        "\n",
        "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
        "                                     role=role,\n",
        "                                     instance_type='ml.m5.large',\n",
        "                                     instance_count=1)\n",
        "\n",
        "\n",
        "# You will need to replace the 'source' code with the location of the dataset you want to process. \n",
        "\n",
        "sklearn_processor.run(code='xgboost_process_script.py',\n",
        "                        inputs=[ProcessingInput(\n",
        "                        source='s3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/train.csv',\n",
        "                        destination='/opt/ml/processing/input/data/')],\n",
        "                      outputs=[ProcessingOutput(source='/opt/ml/processing/output')]\n",
        "                     )\n"
      ],
      "metadata": {
        "id": "ivCa-ePLNzQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#jupyter notebook magic to download data\n",
        "\n",
        "%mkdir ../data\n",
        "!wget -O ../data/aclImdb_v1.tar.gz https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW04iTc0Y2en",
        "outputId": "c0eb45bd-98d1-40c8-b11d-7a5145919b54"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-14 07:01:04--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
            "\n",
            "../data/aclImdb_v1. 100%[===================>]  80.23M  47.1MB/s    in 1.7s    \n",
            "\n",
            "2022-01-14 07:01:05 (47.1 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "def read_imdb_data(data_dir='../data/aclImdb'):\n",
        "    data = {}\n",
        "    labels = {}\n",
        "    \n",
        "    for data_type in ['train', 'test']:\n",
        "        data[data_type] = {}\n",
        "        labels[data_type] = {}\n",
        "        \n",
        "        for sentiment in ['pos', 'neg']:\n",
        "            data[data_type][sentiment] = []\n",
        "            labels[data_type][sentiment] = []\n",
        "            \n",
        "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
        "            files = glob.glob(path)\n",
        "            \n",
        "            for f in files:\n",
        "                with open(f) as review:\n",
        "                    data[data_type][sentiment].append(review.read())\n",
        "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
        "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
        "                    \n",
        "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
        "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
        "                \n",
        "    return data, labels"
      ],
      "metadata": {
        "id": "TTZb0mwfZ1nO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, labels = read_imdb_data()\n",
        "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
        "            len(data['train']['pos']), len(data['train']['neg']),\n",
        "            len(data['test']['pos']), len(data['test']['neg'])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxEPd2yfZ6Go",
        "outputId": "35b865e7-d0e2-4228-8fad-8a627e6ddcc4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def prepare_imdb_data(data, labels):\n",
        "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
        "    \n",
        "    #Combine positive and negative reviews and labels\n",
        "    data_train = data['train']['pos'] + data['train']['neg']\n",
        "    data_test = data['test']['pos'] + data['test']['neg']\n",
        "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
        "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
        "    \n",
        "    #Shuffle reviews and corresponding labels within training and test sets\n",
        "    data_train, labels_train = shuffle(data_train, labels_train)\n",
        "    data_test, labels_test = shuffle(data_test, labels_test)\n",
        "    \n",
        "    # Return a unified training data, test data, training labels, test labets\n",
        "    return data_train, data_test, labels_train, labels_test"
      ],
      "metadata": {
        "id": "sIyF8jNjaD6n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
        "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eAc2k8OaIrt",
        "outputId": "0d2879b4-7aaa-4f34-8458-a46fd3ad337d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMDb reviews (combined): train = 25000, test = 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "EUlprP0PaO9L",
        "outputId": "64f4c43b-d701-4289-d285-67b7893aa891"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'My title ought to be enough.<br /><br />It baffles me that a culture so rich in literary excellence (Dumas, Flaubert, Balzac, Maupassant) would churn out such tosh as the \"nouvelle vague\" cinematic movement. Until the 20th century, France had a great tradition of artistic lucidity and clever philosophy. But the minute you hand them a movie camera they start acting like WOOOHOOO LOOK HOW WEIRD I CAN BE! PLOT? THEME? PSHAW! LET\\'S FILM AN AMUSEMENT PARK RIDE GOING ROUND & ROUND! At least this is not as bad as Godard (who has an unhealthy fascination with the backs of peoples\\' heads. Oh-la-la, quel artiste.). No, Truffaut maintains a degree of visual clarity. But so does the security camera at a quickie-mart. The two are indistinguishable.<br /><br />Haha, just as an aside to all you dweeby film school nerds: I bet the vein is popping out the side of your neck right now. But don\\'t leave without reading the last sentence of my review.<br /><br />Anyway, if you like French literature, you will HATE this. People who like this movie probably have never read any books other than the ramblings of Jack Kerouac or maybe \"Hitchhiker\\'s Guide to the Galaxy\". Or maybe they have read the lyrics to The Doors songs, and they think that\\'s profoundly moving. Whatever floats yer boat. I find it ironic that this film injects some (weak) allusions to Balzac, one of the finest and most meaningful writers who ever lived. Nice try, Truffles. But you\\'re nowhere near the ballpark.<br /><br />Avoid this film like an aids-infected syringe.<br /><br />If you\\'re the type of person who likes to think, then stick to Jean Cocteau (ORPHEE), Robert Bresson (PICKPOCKET) and the Japanese masters Kurosawa (IKURU), Kobayashi (KAIDAN) and Teshigahara (SUNA NO ONNA).<br /><br />If you\\'re an idiot, enjoy your Truffaut, Godard, and Andy Worhol. And for pete\\'s sake push that vein back in your neck. You look like a cabbage.'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZyuknfbaTq0",
        "outputId": "8d2fffd2-f899-4dbb-8a3c-201b29efd2fd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def review_to_words(review):\n",
        "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
        "    words = text.split() # Split string into words\n",
        "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
        "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
        "    \n",
        "    return words"
      ],
      "metadata": {
        "id": "sz5XUcAGaZHC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\n",
        "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
        "\n",
        "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
        "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
        "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
        "\n",
        "    # If cache_file is not None, try to read from it first\n",
        "    cache_data = None\n",
        "    if cache_file is not None:\n",
        "        try:\n",
        "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
        "                cache_data = pickle.load(f)\n",
        "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
        "        except:\n",
        "            pass  # unable to read from cache, but that's okay\n",
        "    \n",
        "    # If cache is missing, then do the heavy lifting\n",
        "    if cache_data is None:\n",
        "        # Preprocess training and test data to obtain words for each review\n",
        "        #words_train = list(map(review_to_words, data_train))\n",
        "        #words_test = list(map(review_to_words, data_test))\n",
        "        words_train = [review_to_words(review) for review in data_train]\n",
        "        words_test = [review_to_words(review) for review in data_test]\n",
        "        \n",
        "        # Write to cache file for future runs\n",
        "        if cache_file is not None:\n",
        "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
        "                              labels_train=labels_train, labels_test=labels_test)\n",
        "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
        "                pickle.dump(cache_data, f)\n",
        "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
        "    else:\n",
        "        # Unpack data loaded from cache file\n",
        "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
        "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
        "    \n",
        "    return words_train, words_test, labels_train, labels_test"
      ],
      "metadata": {
        "id": "H-zQ4KiMacmY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03vrBTqsaojD",
        "outputId": "3b14283a-17c2-46fd-af9c-a1508ec7a58b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote preprocessed data to cache file: preprocessed_data.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import joblib\n",
        "# joblib is an enhanced version of pickle that is more efficient for storing NumPy arrays\n",
        "\n",
        "def extract_BoW_features(words_train, words_test, vocabulary_size=5000,\n",
        "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
        "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
        "    \n",
        "    # If cache_file is not None, try to read from it first\n",
        "    cache_data = None\n",
        "    if cache_file is not None:\n",
        "        try:\n",
        "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
        "                cache_data = joblib.load(f)\n",
        "            print(\"Read features from cache file:\", cache_file)\n",
        "        except:\n",
        "            pass  # unable to read from cache, but that's okay\n",
        "    \n",
        "    # If cache is missing, then do the heavy lifting\n",
        "    if cache_data is None:\n",
        "        # Fit a vectorizer to training documents and use it to transform them\n",
        "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
        "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
        "        vectorizer = CountVectorizer(max_features=vocabulary_size,\n",
        "                preprocessor=lambda x: x, tokenizer=lambda x: x)  # already preprocessed\n",
        "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
        "\n",
        "        # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
        "        features_test = vectorizer.transform(words_test).toarray()\n",
        "        \n",
        "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
        "        \n",
        "        # Write to cache file for future runs (store vocabulary as well)\n",
        "        if cache_file is not None:\n",
        "            vocabulary = vectorizer.vocabulary_\n",
        "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
        "                             vocabulary=vocabulary)\n",
        "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
        "                joblib.dump(cache_data, f)\n",
        "            print(\"Wrote features to cache file:\", cache_file)\n",
        "    else:\n",
        "        # Unpack data loaded from cache file\n",
        "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
        "                cache_data['features_test'], cache_data['vocabulary'])\n",
        "    \n",
        "    # Return both the extracted features as well as the vocabulary\n",
        "    return features_train, features_test, vocabulary"
      ],
      "metadata": {
        "id": "N_NZeYPVat4b"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Bag of Words features for both training and test datasets\n",
        "train_X, test_X, vocabulary = extract_BoW_features(train_X, test_X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA7zB3b7a1Yh",
        "outputId": "8838ea92-be7b-4fb9-a08b-e81c67cf2a96"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote features to cache file: bow_features.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aN_DYzqJa5xw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}